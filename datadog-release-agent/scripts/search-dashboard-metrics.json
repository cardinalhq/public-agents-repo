{
  "schemaVersion": 1,
  "name": "search-dashboard-metrics",
  "description": "",
  "language": "python",
  "code": "import sys\nimport json\nimport os\nimport re\nfrom datetime import datetime, timedelta, timezone\nimport requests\n\n\n# =============================================================================\n# Logging\n# =============================================================================\n\ndef eprint(msg: str):\n    print(msg, file=sys.stderr)\n\n\ndef _json_preview(obj, max_len: int = 2500) -\u003e str:\n    try:\n        s = json.dumps(obj, ensure_ascii=False, sort_keys=True, indent=2)\n    except Exception:\n        s = str(obj)\n    return s if len(s) \u003c= max_len else s[:max_len] + \"\\n...(truncated)\"\n\n\ndef redact_secrets(obj):\n    \"\"\"\n    Recursively redact common secret fields so we can safely log payloads.\n    \"\"\"\n    secret_keys = {\n        \"dd_api_key\", \"dd_app_key\", \"api_key\", \"app_key\", \"token\",\n        \"authorization\", \"cardinalhq_api_key\", \"slack_bot_token\",\n        \"slack_token\", \"bearer\", \"x-api-key\"\n    }\n\n    if isinstance(obj, dict):\n        out = {}\n        for k, v in obj.items():\n            lk = (k or \"\").lower()\n            if lk in secret_keys:\n                out[k] = \"***REDACTED***\"\n            else:\n                out[k] = redact_secrets(v)\n        return out\n    if isinstance(obj, list):\n        return [redact_secrets(x) for x in obj]\n    return obj\n\n\n# =============================================================================\n# Time / Parsing\n# =============================================================================\n\ndef parse_iso8601(ts: str):\n    if not ts:\n        return None\n    ts = ts.replace(\"Z\", \"+00:00\")\n    dt = datetime.fromisoformat(ts)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\n\ndef _iso_z(dt: datetime) -\u003e str:\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n\n\ndef parse_time_window_to_timedelta(s: str) -\u003e timedelta:\n    if not s:\n        return timedelta(hours=1)\n    raw = s.strip().lower()\n\n    m = re.match(r\"^\\s*(\\d+)\\s*([mh])\\s*$\", raw)\n    if m:\n        n = int(m.group(1))\n        unit = m.group(2)\n        return timedelta(minutes=n) if unit == \"m\" else timedelta(hours=n)\n\n    m = re.match(r\"^\\s*(\\d+)\\s*(minute|minutes|min|hour|hours|hr|hrs)\\s*$\", raw)\n    if m:\n        n = int(m.group(1))\n        unit = m.group(2)\n        if unit in (\"minute\", \"minutes\", \"min\"):\n            return timedelta(minutes=n)\n        return timedelta(hours=n)\n\n    return timedelta(hours=1)\n\n\ndef parse_group_by_from_datadog_query(query: str):\n    \"\"\"\n    Extract group-by keys from 'by {a,b}' in Datadog query.\n    Defaults to ['env'] if missing.\n    \"\"\"\n    m = re.search(r\"\\bby\\s*\\{([^}]+)\\}\", query or \"\")\n    if not m:\n        return [\"env\"]\n    parts = [p.strip() for p in m.group(1).split(\",\") if p.strip()]\n    return parts or [\"env\"]\n\n\n# =============================================================================\n# Metric extraction / payload shaping\n# =============================================================================\n\ndef extract_metric_name(query: str) -\u003e str:\n    \"\"\"\n    Extract metric name from a Datadog query string.\n    Example: \"sum:paymentservice.payment_processing_failures{*} by {cardtype}.as_count()\"\n    Returns: \"paymentservice.payment_processing_failures\"\n    \"\"\"\n    if not query:\n        return \"unknown_metric\"\n\n    q = query.strip()\n\n    # remove leading \"sum:\", \"avg:\", etc.\n    if \":\" in q:\n        parts = q.split(\":\", 1)\n        if len(parts) \u003e 1:\n            q = parts[1]\n\n    # metric name ends before \"{\" or \"(\"\n    metric_name = q.split(\"{\")[0].split(\"(\")[0].strip()\n    metric_name = metric_name.rstrip(\". \")\n\n    return metric_name or \"unknown_metric\"\n\n\ndef build_metrics(dashboards: list, release_dt: datetime, window_td: timedelta, step_seconds: int) -\u003e list:\n    \"\"\"\n    Build metrics[] where each item contains:\n      - metric_name\n      - query_execution_info (includes datasource_type + query_type + group_by)\n      - dashboard (metadata struct)\n    \"\"\"\n    metrics = []\n\n    start_dt = release_dt - window_td\n    end_dt = release_dt + window_td\n\n    for i, dash in enumerate(dashboards or []):\n        query = dash.get(\"expression\") or \"\"\n        dash_name = dash.get(\"dashboard_name\") or dash.get(\"title\") or \"unknown\"\n\n        if not query:\n            eprint(f\"[build_metrics] skip idx={i} dashboard={dash_name} reason=no_expression\")\n            continue\n\n        metric_name = extract_metric_name(query)\n        group_by = parse_group_by_from_datadog_query(query)\n\n        query_execution_info = {\n            \"datasource_type\": \"datadog\",\n            \"query_type\": \"aggregate\",\n            \"query\": query,\n            \"start_seconds\": int(start_dt.timestamp()),\n            \"end_seconds\": int(end_dt.timestamp()),\n            \"step_seconds\": int(step_seconds),\n            # helpful extras\n            \"metric_name\": metric_name,\n            \"group_by\": group_by,\n        }\n\n        dashboard = {\n            \"dashboard_id\": dash.get(\"dashboard_id\", \"\"),\n            \"dashboard_name\": dash.get(\"dashboard_name\", \"\"),\n            \"dashboard_url\": dash.get(\"dashboard_url\", \"\"),\n            \"expression\": query,\n            \"similarity\": dash.get(\"similarity\", 0.0),\n        }\n\n        metric_obj = {\n            \"metric_name\": metric_name,\n            \"query_execution_info\": query_execution_info,\n            \"dashboard\": dashboard,\n        }\n\n        eprint(\n            f\"[build_metrics] +metric idx={i} metric_name={metric_name} \"\n            f\"dashboard_name={dashboard.get('dashboard_name')} url={dashboard.get('dashboard_url')}\"\n        )\n        eprint(f\"[build_metrics]   query_execution_info: {_json_preview(query_execution_info)}\")\n        eprint(f\"[build_metrics]   dashboard: {_json_preview(dashboard)}\")\n\n        metrics.append(metric_obj)\n\n    return metrics\n\n\n# =============================================================================\n# Script invocation\n# =============================================================================\n\ndef call_timeseries_script(payload: dict) -\u003e dict:\n    from cardinal_runtime import call_script\n\n    eprint(f\"[invoke] calling generate-metric-analysis metrics_len={len(payload.get('metrics') or [])}\")\n    eprint(f\"[invoke] payload (redacted): {_json_preview(redact_secrets(payload))}\")\n\n    return call_script(\"generate-metric-analysis\", payload)\n\n\ndef run(input_data: dict) -\u003e dict:\n    service_name = input_data.get(\"service_name\")\n    release_time = input_data.get(\"release_time\")\n    time_window_str = input_data.get(\"time_window\", \"1h\")\n    step_seconds = int(input_data.get(\"step_seconds\", 60))\n    max_dashboards = int(input_data.get(\"max_dashboards\", 10))\n\n    if not service_name:\n        return {\"ok\": False, \"error_code\": \"INVALID_INPUT\", \"error_message\": \"service_name is required\"}\n    if not release_time:\n        return {\"ok\": False, \"error_code\": \"INVALID_INPUT\", \"error_message\": \"release_time is required\"}\n\n    release_dt = parse_iso8601(release_time)\n    if not release_dt:\n        return {\"ok\": False, \"error_code\": \"INVALID_INPUT\", \"error_message\": \"release_time invalid\"}\n\n    window_td = parse_time_window_to_timedelta(time_window_str)\n\n    local_api_base = (os.environ.get(\"LOCAL_API_BASE_URL\", \"http://localhost:20202\")).rstrip(\"/\")\n    local_api_token = os.environ.get(\"CARDINALHQ_API_KEY\")\n\n    dd_api_key = os.environ.get(\"DATADOG_API_KEY\")\n    dd_app_key = os.environ.get(\"DATADOG_APP_KEY\")\n    dd_site = os.environ.get(\"DATADOG_SITE\") or \"datadoghq.com\"\n\n    slack_bot_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n    slack_channel_id = os.environ.get(\"SLACK_CHANNEL_ID\")  # optional\n\n    # We still require these in the search script because we need them to search dashboards.\n    missing = [k for k, v in {\n        \"CARDINALHQ_API_KEY\": local_api_token,\n        \"DATADOG_API_KEY\": dd_api_key,\n        \"DATADOG_APP_KEY\": dd_app_key,\n        # slack token is only required by the timeseries script, but we'll pass it if present\n    }.items() if not v]\n    if missing:\n        return {\n            \"ok\": False,\n            \"error_code\": \"MISSING_ENV_VARS\",\n            \"error_message\": f\"Missing env vars: {', '.join(missing)}\",\n        }\n\n    # 1) Search dashboards\n    search_url = f\"{local_api_base}/api/v1/dashboards/search\"\n    search_payload = {\n        \"datasource_type\": \"datadog\",\n        \"question\": service_name,\n        \"data_type\": \"metrics\",\n    }\n\n    eprint(f\"[search] POST {search_url}\")\n    eprint(f\"[search] payload (redacted): {_json_preview(redact_secrets(search_payload))}\")\n\n    resp = requests.post(search_url, json=search_payload, \n                         headers={\n                           \"Content-Type\": \"application/json\",\n                           \"DATADOG_API_KEY\": dd_api_key,\n                           \"DATADOG_APP_KEY\": dd_app_key,\n                           \"DATADOG_SITE\": dd_site\n                         }, \n                         timeout=30)\n    resp.raise_for_status()\n    data = resp.json()\n\n    eprint(f\"[search] response status: {resp.status_code}\")\n    eprint(f\"[search] full response: {_json_preview(data)}\")\n\n    dashboards = data.get(\"dashboards\", data.get(\"charts\", [])) or []\n    eprint(f\"[search] found {len(dashboards)} dashboards total\")\n    dashboards = dashboards[:max_dashboards]\n    eprint(f\"[search] using first {len(dashboards)} dashboards (max_dashboards={max_dashboards})\")\n\n    if dashboards:\n        eprint(f\"[search] first dashboard: {_json_preview(dashboards[0])}\")\n\n    # 2) Build metrics[]\n    metrics = build_metrics(dashboards, release_dt, window_td, step_seconds)\n    eprint(f\"[build_metrics] created {len(metrics)} metrics\")\n\n    # 3) Build the payload being sent (include secrets since env vars don't propagate through script call)\n    ts_payload = {\n        \"service_name\": service_name,\n        \"release_time\": _iso_z(release_dt),\n        \"time_window\": time_window_str,\n        \"metrics\": metrics,\n        \"metric_specs\": [],\n        # Pass secrets in payload so called script has access to them\n        \"cardinalhq_api_key\": local_api_token,\n        \"dd_api_key\": dd_api_key,\n        \"dd_app_key\": dd_app_key,\n        \"dd_site\": dd_site,\n        \"slack_bot_token\": slack_bot_token,\n        \"slack_channel_id\": slack_channel_id,\n    }\n\n    eprint(f\"[ts_payload] metrics_len={len(metrics)}\")\n    eprint(f\"[ts_payload] payload: {_json_preview(ts_payload)}\")\n\n    # 4) Call timeseries script\n    ts_out = call_timeseries_script(ts_payload)\n\n    return {\n        \"ok\": True,\n        \"service_name\": service_name,\n        \"release_time\": _iso_z(release_dt),\n        \"time_window\": time_window_str,\n        \"step_seconds\": step_seconds,\n        \"dashboards_found\": len(dashboards),\n        \"metrics_sent\": len(metrics),\n        \"timeseries_result\": ts_out,\n    }\n\n\nif __name__ == \"__main__\":\n    try:\n        input_text = sys.stdin.read()\n        input_data = json.loads(input_text or \"{}\")\n        result = run(input_data)\n        print(json.dumps(result, indent=2))\n    except Exception as e:\n        import traceback\n        eprint(str(e))\n        traceback.print_exc(file=sys.stderr)\n        print(json.dumps({\"ok\": False, \"error_code\": \"MAIN_ERROR\", \"error_message\": str(e)}))\n        sys.exit(1)",
  "environmentVariables": [],
  "inputSchema": {
    "properties": {
      "max_dashboards": {
        "description": "Maximum number of dashboards to process",
        "type": "integer"
      },
      "release_time": {
        "description": "RFC3339 timestamp for the release time",
        "type": "string"
      },
      "service_name": {
        "description": "The service to search for in dashboards",
        "type": "string"
      },
      "step_seconds": {
        "description": "Step interval in seconds for time series data",
        "type": "integer"
      },
      "time_window": {
        "description": "Time window string (e.g., '1h', '30m')",
        "type": "string"
      }
    },
    "required": [
      "service_name",
      "release_time"
    ],
    "type": "object"
  },
  "dependencies": {
    "enabled": true,
    "manager": "",
    "specText": "matplotlib\u003e=3.5.0\nnumpy\u003e=1.21.0\nrequests\u003e=2.28.0",
    "installPolicy": "",
    "installScope": "",
    "cacheEnabled": false,
    "lastInstall": {
      "status": "success",
      "installedAt": "2026-01-09T23:21:52Z",
      "cacheKey": "46dd0f0b8194d47dfefc74272b987232931d9c9d4405aa30ca9ed2a9ca71b316",
      "durationMs": 8016
    }
  },
  "execution": {
    "timeoutSeconds": 30,
    "workingDirectory": "workspace"
  },
  "createdAt": "2026-01-01T00:08:49Z",
  "updatedAt": "2026-01-14T22:51:54Z"
}