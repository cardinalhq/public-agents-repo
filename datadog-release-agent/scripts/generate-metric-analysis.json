{
  "schemaVersion": 1,
  "name": "generate-metric-analysis",
  "description": "",
  "language": "python",
  "code": "import sys\nimport json\nimport os\nimport re\nimport requests\nfrom datetime import datetime, timedelta, timezone\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom io import BytesIO\n\n\nSTRICT_STDOUT_JSON_ONLY = True\n\n\ndef eprint(msg: str):\n    print(msg, file=sys.stderr)\n\n\ndef safe_filename(name: str, default: str = \"chart.png\") -\u003e str:\n    if not name:\n        return default\n    s = name.strip()\n    s = re.sub(r\"[\\\\/]+\", \"_\", s)\n    s = re.sub(r\"[^a-zA-Z0-9._()-]+\", \"_\", s)\n    s = s.strip(\"_\")\n    if not s:\n        return default\n    if not s.lower().endswith(\".png\"):\n        s += \".png\"\n    return s\n\n\ndef parse_iso8601(ts: str):\n    if not ts:\n        return None\n    ts = ts.replace(\"Z\", \"+00:00\")\n    dt = datetime.fromisoformat(ts)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\n\ndef parse_time_window_to_timedelta(s: str) -\u003e timedelta:\n    if not s:\n        return timedelta(hours=1)\n    raw = s.strip().lower()\n    m = re.match(r\"^\\s*(\\d+)\\s*([mh])\\s*$\", raw)\n    if m:\n        n = int(m.group(1))\n        unit = m.group(2)\n        return timedelta(minutes=n) if unit == \"m\" else timedelta(hours=n)\n    m = re.match(r\"^\\s*(\\d+)\\s*(minute|minutes|min|hour|hours|hr|hrs)\\s*$\", raw)\n    if m:\n        n = int(m.group(1))\n        unit = m.group(2)\n        if unit in (\"minute\", \"minutes\", \"min\"):\n            return timedelta(minutes=n)\n        return timedelta(hours=n)\n    return timedelta(hours=1)\n\n\ndef datadog_response_to_series(response_data):\n    \"\"\"\n    Convert Datadog API response to series_map format for plotting.\n    Handles the pointlist format from Datadog.\n    Preserves group information from scope field for grouped metrics.\n    \"\"\"\n    series_map = {}\n\n    series_list = response_data.get(\"series\", [])\n    for series in series_list:\n        pointlist = series.get(\"pointlist\", [])\n        \n        # Use scope field for label if available (grouped metrics)\n        scope = series.get(\"scope\", \"\")\n        metric_name = series.get(\"metric\", \"metric\")\n        \n        if scope:\n            parts = scope.split(\":\")\n            if len(parts) \u003e 1:\n                label = \":\".join(parts[1:])  # Extract value part after key:\n            else:\n                label = scope\n        else:\n            label = metric_name\n\n        # Extract points as (datetime, value) tuples\n        points = []\n        for point in pointlist:\n            if len(point) \u003e= 2:\n                ts_ms = point[0]\n                value = point[1]\n                if value is not None:\n                    # Convert milliseconds to seconds\n                    ts_sec = ts_ms / 1000.0\n                    dt = datetime.fromtimestamp(ts_sec, tz=timezone.utc)\n                    points.append((dt, float(value)))\n\n        if points:\n            series_map[label] = points\n\n    return series_map\n\n\ndef plot_continuous_with_release_boundary(\n    metric_name: str,\n    pre_series_map: dict,\n    post_series_map: dict,\n    release_dt: datetime,\n    y_label: str,\n):\n    fig, ax = plt.subplots(figsize=(13, 6))\n    all_labels = sorted(set((pre_series_map or {}).keys()) | set((post_series_map or {}).keys()))\n    color_cycle = plt.rcParams[\"axes.prop_cycle\"].by_key().get(\"color\", [\"#1f77b4\"])\n    color_map = {lbl: color_cycle[i % len(color_cycle)] for i, lbl in enumerate(all_labels)}\n\n    def plot_series(series_map, suffix):\n        for label, pts in (series_map or {}).items():\n            if not pts:\n                continue\n            xs = [p[0] for p in pts]\n            ys = [p[1] for p in pts]\n            ax.plot(xs, ys, linewidth=2.0, linestyle=\"-\", color=color_map.get(label), alpha=0.95, label=f\"{label} — {suffix}\")\n\n    plot_series(pre_series_map, \"Pre-release\")\n    plot_series(post_series_map, \"Post-release\")\n    ax.axvline(x=release_dt, color=\"red\", linestyle=\"--\", linewidth=2.8, label=\"Release\", alpha=0.95, zorder=3)\n    ax.set_title(f\"{metric_name}\\nPre vs Post Release Comparison\", fontsize=14, fontweight=\"bold\", pad=12)\n    ax.set_xlabel(\"Time (UTC)\", fontsize=11)\n    ax.set_ylabel(y_label or \"value\", fontsize=11)\n    ax.grid(True, alpha=0.25)\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\", tz=timezone.utc))\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(45)\n        tick.set_ha(\"right\")\n    trans = ax.get_xaxis_transform()\n    ax.text(release_dt - timedelta(minutes=1), 0.98, \"Pre\", transform=trans, ha=\"right\", va=\"top\", fontsize=11, fontweight=\"bold\")\n    ax.text(release_dt + timedelta(minutes=1), 0.98, \"Post\", transform=trans, ha=\"left\", va=\"top\", fontsize=11, fontweight=\"bold\")\n    ax.legend(loc=\"best\", fontsize=9)\n    plt.tight_layout()\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", dpi=140, bbox_inches=\"tight\")\n    buf.seek(0)\n    plt.close(fig)\n    return buf\n\n\ndef upload_image_to_slack_external(image_buffer: BytesIO, filename: str, title: str = None, channel_id: str = None, slack_token: str = None):\n    if not slack_token:\n        slack_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n    if not slack_token:\n        raise ValueError(\"Missing SLACK_BOT_TOKEN in payload or environment variables\")\n    \n    filename = safe_filename(filename)\n    image_buffer.seek(0)\n    img_bytes = image_buffer.read()\n    file_len = int(len(img_bytes))\n    \n    headers = {\"Authorization\": f\"Bearer {slack_token}\"}\n    \n    # Step 1: getUploadURLExternal (use data= form-encoded like your working example)\n    r1 = requests.post(\n        \"https://slack.com/api/files.getUploadURLExternal\",\n        headers=headers,\n        data={\"filename\": filename, \"length\": file_len},\n        timeout=30\n    )\n    j1 = r1.json()\n    if not j1.get(\"ok\"):\n        meta = j1.get(\"response_metadata\") or {}\n        msgs = meta.get(\"messages\") or []\n        if msgs:\n            eprint(f\"Slack getUploadURLExternal messages: {msgs}\")\n        eprint(f\"Slack getUploadURLExternal full response (first 1500): {json.dumps(j1, indent=2)[:1500]}\")\n        raise ValueError(f\"Slack getUploadURLExternal failed: {j1.get('error')}\")\n    \n    upload_url = j1[\"upload_url\"]\n    file_id = j1[\"file_id\"]\n    \n    # Step 2: upload to the returned URL\n    r2 = requests.post(\n        upload_url,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=img_bytes,\n        timeout=60\n    )\n    if r2.status_code \u003e= 300:\n        raise ValueError(f\"Upload to Slack external URL failed: {r2.status_code} {r2.text[:300]}\")\n    \n    # Step 3: completeUploadExternal (use data= and files as JSON string)\n    payload = {\"files\": json.dumps([{\"id\": file_id, \"title\": title or filename}])}\n    if channel_id:\n        payload[\"channel_id\"] = channel_id\n    \n    r3 = requests.post(\n        \"https://slack.com/api/files.completeUploadExternal\",\n        headers=headers,\n        data=payload,\n        timeout=30\n    )\n    j3 = r3.json()\n    if not j3.get(\"ok\"):\n        meta = j3.get(\"response_metadata\") or {}\n        msgs = meta.get(\"messages\") or []\n        if msgs:\n            eprint(f\"Slack completeUploadExternal messages: {msgs}\")\n        eprint(f\"Slack completeUploadExternal full response (first 1500): {json.dumps(j3, indent=2)[:1500]}\")\n        raise ValueError(f\"Slack completeUploadExternal failed: {j3.get('error')}\")\n    \n    # Return file_id for BlockKit slack_file reference\n    return file_id\n\n\ndef query_datadog_timeseries(api_key, app_key, query, start_dt, end_dt):\n    \"\"\"\n    Query Datadog timeseries API.\n    Returns the JSON response from Datadog.\n    \"\"\"\n    start_epoch = int(start_dt.timestamp())\n    end_epoch = int(end_dt.timestamp())\n\n    url = \"https://api.us3.datadoghq.com/api/v1/query\"\n    params = {\n        \"query\": query,\n        \"from\": start_epoch,\n        \"to\": end_epoch,\n    }\n    headers = {\n        \"DD-API-KEY\": api_key,\n        \"DD-APPLICATION-KEY\": app_key,\n    }\n\n    eprint(f\"Querying Datadog: {query} from {start_epoch} to {end_epoch}\")\n\n    response = requests.get(url, params=params, headers=headers, timeout=30)\n    response.raise_for_status()\n\n    return response.json()\n\n\ndef extract_values_from_datadog_response(response_data):\n    \"\"\"\n    Extract values per series from Datadog timeseries response.\n    Returns a dict mapping series label -\u003e list of values.\n    This preserves grouping (e.g., by cardtype).\n\n    For grouped queries (e.g., by {cardtype}), each series has the same metric name\n    but different scope (e.g., \"cardtype:mastercard\"). We extract the label from\n    the scope field to distinguish between groups.\n    \"\"\"\n    series_values = {}\n\n    series_list = response_data.get(\"series\", [])\n    eprint(f\"DEBUG: Total series returned: {len(series_list)}\")\n\n    for idx, series in enumerate(series_list):\n        pointlist = series.get(\"pointlist\", [])\n\n        # For grouped metrics, use the scope field to distinguish groups\n        # scope looks like \"cardtype:mastercard\" - extract the value part\n        scope = series.get(\"scope\", \"\")\n        metric_name = series.get(\"metric\", \"metric\")\n\n        # Build label: prefer scope if available (for grouped metrics), else use metric name\n        if scope:\n            # scope format is \"key:value\" - extract the value part after the first colon\n            parts = scope.split(\":\")\n            if len(parts) \u003e 1:\n                label = \":\".join(parts[1:])  # Handle cases with multiple colons\n            else:\n                label = scope\n        else:\n            label = metric_name\n\n        eprint(f\"DEBUG: Series {idx}: metric='{metric_name}', scope='{scope}', label='{label}', points={len(pointlist)}\")\n        eprint(f\"DEBUG: Series {idx} structure: {json.dumps({k: v for k, v in series.items() if k != 'pointlist'}, indent=2)}\")\n\n        values = []\n        for point in pointlist:\n            if len(point) \u003e= 2 and point[1] is not None:\n                values.append(float(point[1]))\n\n        eprint(f\"DEBUG: Series {idx} extracted {len(values)} values\")\n\n        if values:\n            series_values[label] = values\n\n    eprint(f\"DEBUG: Final series_values keys: {list(series_values.keys())}\")\n    return series_values\n\n\ndef calculate_percentile(values, percentile):\n    \"\"\"\n    Calculate the given percentile from a list of values.\n    Uses linear interpolation.\n    \"\"\"\n    if not values:\n        return 0.0\n\n    sorted_values = sorted(values)\n    n = len(sorted_values)\n\n    if n == 1:\n        return sorted_values[0]\n\n    index = (percentile / 100.0) * (n - 1)\n    lower_index = int(index)\n    upper_index = min(lower_index + 1, n - 1)\n\n    if lower_index == upper_index:\n        return sorted_values[lower_index]\n\n    fraction = index - lower_index\n    return sorted_values[lower_index] + fraction * (sorted_values[upper_index] - sorted_values[lower_index])\n\n\ndef generate_slack_blocks(service_name, release_time, time_window, results_by_type, chart_file_data=None):\n    \"\"\"\n    Generate Slack blocks for the release analysis report.\n    Groups results by analysis_type and formats them nicely.\n    \n    chart_file_data: dict mapping metric_name -\u003e file_id (Slack file ID)\n    \"\"\"\n    if chart_file_data is None:\n        chart_file_data = {}\n\n    blocks = []\n\n    blocks.append({\n        \"type\": \"header\",\n        \"text\": {\n            \"type\": \"plain_text\",\n            \"text\": f\"Release Analysis: {service_name}\",\n        }\n    })\n\n    blocks.append({\n        \"type\": \"section\",\n        \"text\": {\n            \"type\": \"mrkdwn\",\n            \"text\": f\"*Release Time:* {release_time}\\n*Time Window:* {time_window}\",\n        }\n    })\n\n    blocks.append({\"type\": \"divider\"})\n\n    analysis_type_labels = {\n        \"SYSTEM\": \"System Metrics\",\n        \"RED\": \"RED Metrics\",\n        \"CUSTOM\": \"Custom Metrics\",\n    }\n\n    for analysis_type in [\"SYSTEM\", \"RED\", \"CUSTOM\"]:\n        results = results_by_type.get(analysis_type, [])\n\n        if not results:\n            continue\n\n        blocks.append({\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": f\"*{analysis_type_labels[analysis_type]}*\",\n            }\n        })\n\n        # Track which metric names we've added charts for\n        charts_added = set()\n\n        for result in results:\n            metric_name = result[\"metric_name\"]\n            base_metric_name = result.get(\"base_metric_name\", metric_name.split(\" (\")[0])\n            before_p50 = result[\"before_p50\"]\n            before_p99 = result[\"before_p99\"]\n            after_p50 = result[\"after_p50\"]\n            after_p99 = result[\"after_p99\"]\n            p50_change = result[\"p50_change\"]\n            p99_change = result[\"p99_change\"]\n            p50_increase = result[\"p50_increase\"]\n            p99_increase = result[\"p99_increase\"]\n\n            p50_emoji = \":arrow_up:\" if p50_increase else \":arrow_right:\"\n            p99_emoji = \":arrow_up:\" if p99_increase else \":arrow_right:\"\n\n            text = f\"*{metric_name}*\\n\"\n            text += f\"• P50: {before_p50:.2f} → {after_p50:.2f} {p50_emoji} ({p50_change:+.1f}%)\\n\"\n            text += f\"• P99: {before_p99:.2f} → {after_p99:.2f} {p99_emoji} ({p99_change:+.1f}%)\"\n\n            blocks.append({\n                \"type\": \"section\",\n                \"text\": {\n                    \"type\": \"mrkdwn\",\n                    \"text\": text,\n                }\n            })\n\n            # Add chart if available and not yet added for this base metric\n            if base_metric_name in chart_file_data and base_metric_name not in charts_added:\n                file_id = chart_file_data[base_metric_name]\n                \n                if file_id:\n                    blocks.append({\n                        \"type\": \"image\",\n                        \"slack_file\": {\"id\": file_id},\n                        \"alt_text\": f\"{base_metric_name} pre vs post release chart\"\n                    })\n                    charts_added.add(base_metric_name)\n                else:\n                    eprint(f\"Warning: No file ID available for chart {base_metric_name}, skipping image block\")\n\n        blocks.append({\"type\": \"divider\"})\n\n    if not any(results_by_type.values()):\n        blocks.append({\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": \"_No metrics showed post-release increases._\",\n            }\n        })\n\n    return blocks\n\n\ndef run(input_data):\n    try:\n        service_name = input_data.get(\"service_name\", \"Unknown Service\")\n        release_time = input_data.get(\"release_time\")\n        time_window_str = input_data.get(\"time_window\", \"1 hour\")\n        \n        # Accept EITHER metric_specs OR metrics (from dashboard)\n        metric_specs = input_data.get(\"metric_specs\", [])\n        metrics = input_data.get(\"metrics\", [])\n        slack_channel_id = input_data.get(\"slack_channel_id\")\n        slack_bot_token = input_data.get(\"slack_bot_token\")\n\n        if not release_time:\n            raise ValueError(\"Input is missing 'release_time'\")\n        \n        # At least one of metric_specs or metrics must be provided\n        if not metric_specs and not metrics:\n            raise ValueError(\"Input is missing both 'metric_specs' and 'metrics'\")\n\n        release_dt = parse_iso8601(release_time)\n        if not release_dt:\n            raise ValueError(\"Invalid 'release_time' format\")\n\n        window_td = parse_time_window_to_timedelta(time_window_str)\n\n        # Try to get API keys from payload first (for dashboard flow), then from environment (for direct workflow calls)\n        dd_api_key = input_data.get(\"dd_api_key\") or os.environ.get(\"DATADOG_API_KEY\")\n        dd_app_key = input_data.get(\"dd_app_key\") or os.environ.get(\"DATADOG_APP_KEY\")\n\n        if not dd_api_key or not dd_app_key:\n            raise ValueError(\"Missing DATADOG_API_KEY or DATADOG_APP_KEY in payload or environment variables\")\n\n        before_start = release_dt - window_td\n        before_end = release_dt\n        after_start = release_dt\n        after_end = release_dt + window_td\n\n        eprint(f\"Analyzing release for {service_name} at {release_time}\")\n        eprint(f\"Before window: {before_start.isoformat()} to {before_end.isoformat()}\")\n        eprint(f\"After window: {after_start.isoformat()} to {after_end.isoformat()}\")\n\n        metrics_analyzed = 0\n        metrics_with_increases = 0\n        results_by_type = {\"SYSTEM\": [], \"RED\": [], \"CUSTOM\": []}\n        chart_file_data = {}\n        charts_generated = set()\n\n        # Process metric_specs (from metrics-release-workflow)\n        for idx, metric_spec in enumerate(metric_specs):\n            metric_name = metric_spec.get(\"metric_name\", f\"metric_{idx+1}\")\n            query_expression = metric_spec.get(\"query_expression\", \"\")\n            analysis_type = metric_spec.get(\"analysis_type\", \"CUSTOM\")\n\n            if not query_expression:\n                eprint(f\"Skipping metric '{metric_name}': no query_expression\")\n                continue\n\n            eprint(f\"Processing metric_spec {idx+1}/{len(metric_specs)}: {metric_name}\")\n\n            try:\n                before_data = query_datadog_timeseries(\n                    dd_api_key, dd_app_key, query_expression, before_start, before_end\n                )\n                after_data = query_datadog_timeseries(\n                    dd_api_key, dd_app_key, query_expression, after_start, after_end\n                )\n\n                before_series_values = extract_values_from_datadog_response(before_data)\n                after_series_values = extract_values_from_datadog_response(after_data)\n\n                eprint(f\"DEBUG: Before series: {list(before_series_values.keys())}\")\n                eprint(f\"DEBUG: After series: {list(after_series_values.keys())}\")\n\n                if not before_series_values or not after_series_values:\n                    eprint(f\"Skipping metric '{metric_name}': insufficient data\")\n                    continue\n\n                all_labels = sorted(set(before_series_values.keys()) | set(after_series_values.keys()))\n                eprint(f\"DEBUG: All labels to process: {all_labels}\")\n\n                for label in all_labels:\n                    before_values = before_series_values.get(label, [])\n                    after_values = after_series_values.get(label, [])\n\n                    eprint(f\"DEBUG: Processing label '{label}': before={len(before_values)} values, after={len(after_values)} values\")\n\n                    if not before_values or not after_values:\n                        eprint(f\"DEBUG: Skipping {label} - missing data\")\n                        continue\n\n                    before_p50 = calculate_percentile(before_values, 50)\n                    before_p99 = calculate_percentile(before_values, 99)\n                    after_p50 = calculate_percentile(after_values, 50)\n                    after_p99 = calculate_percentile(after_values, 99)\n\n                    metrics_analyzed += 1\n\n                    # Skip if baseline values are too small to calculate meaningful percentages\n                    if before_p50 \u003c 0.01 or before_p99 \u003c 0.01:\n                        eprint(f\"DEBUG: Skipping {label} - baseline values too small (P50: {before_p50:.4f}, P99: {before_p99:.4f})\")\n                        continue\n\n                    # Calculate percentage changes first\n                    p50_change_pct = ((after_p50 - before_p50) / before_p50 * 100)\n                    p99_change_pct = ((after_p99 - before_p99) / before_p99 * 100)\n\n                    # Check if BOTH P50 and P99 increased by at least 10%\n                    has_significant_increase = (p50_change_pct \u003e= 10.0 and p99_change_pct \u003e= 10.0)\n\n                    eprint(f\"DEBUG: {label} - P50: {before_p50:.2f} -\u003e {after_p50:.2f} ({p50_change_pct:+.1f}%), P99: {before_p99:.2f} -\u003e {after_p99:.2f} ({p99_change_pct:+.1f}%)\")\n                    eprint(f\"DEBUG: {label} - Significant increase (both \u003e= 10%): {has_significant_increase}\")\n\n                    if has_significant_increase:\n                        metrics_with_increases += 1\n\n                        p50_increase = after_p50 \u003e before_p50\n                        p99_increase = after_p99 \u003e before_p99\n\n                        result_metric_name = f\"{metric_name} ({label})\" if label != \"metric\" else metric_name\n\n                        result = {\n                            \"metric_name\": result_metric_name,\n                            \"base_metric_name\": metric_name,\n                            \"before_p50\": before_p50,\n                            \"before_p99\": before_p99,\n                            \"after_p50\": after_p50,\n                            \"after_p99\": after_p99,\n                            \"p50_change\": p50_change_pct,\n                            \"p99_change\": p99_change_pct,\n                            \"p50_increase\": p50_increase,\n                            \"p99_increase\": p99_increase,\n                        }\n\n                        results_by_type[analysis_type].append(result)\n                        eprint(f\"  Found significant increase for {label} - P50: {before_p50:.2f} -\u003e {after_p50:.2f} ({p50_change_pct:+.1f}%), P99: {before_p99:.2f} -\u003e {after_p99:.2f} ({p99_change_pct:+.1f}%)\")\n\n                        if metric_name not in charts_generated:\n                            try:\n                                before_series_map = datadog_response_to_series(before_data)\n                                after_series_map = datadog_response_to_series(after_data)\n\n                                chart_buf = plot_continuous_with_release_boundary(\n                                    metric_name,\n                                    before_series_map,\n                                    after_series_map,\n                                    release_dt,\n                                    f\"{metric_name}\"\n                                )\n\n                                file_id = upload_image_to_slack_external(\n                                    chart_buf,\n                                    f\"{metric_name}_chart\",\n                                    title=f\"{metric_name} - Pre vs Post Release\",\n                                    channel_id=slack_channel_id,\n                                    slack_token=slack_bot_token\n                                )\n                                chart_file_data[metric_name] = file_id\n                                charts_generated.add(metric_name)\n                                eprint(f\"  Uploaded chart to Slack: {file_id}\")\n                            except Exception as chart_err:\n                                eprint(f\"  Warning: Failed to generate/upload chart for '{metric_name}': {chart_err}\")\n                    else:\n                        eprint(f\"  No significant increase detected for {label} (need both P50 and P99 \u003e= 10% increase)\")\n\n            except Exception as e:\n                eprint(f\"Error processing metric_spec '{metric_name}': {e}\")\n                import traceback\n                eprint(traceback.format_exc())\n\n        # Process metrics (from dashboard flow)\n        for idx, metric_config in enumerate(metrics):\n            metric_name = metric_config.get(\"metric_name\", f\"dashboard_metric_{idx+1}\")\n            qei = metric_config.get(\"query_execution_info\", {})\n            query_expression = qei.get(\"query\", \"\")\n            analysis_type = metric_config.get(\"analysis_type\", \"CUSTOM\")\n            dashboard = metric_config.get(\"dashboard\", {})\n\n            if not query_expression:\n                eprint(f\"Skipping dashboard metric '{metric_name}': no query\")\n                continue\n\n            eprint(f\"Processing dashboard metric {idx+1}/{len(metrics)}: {metric_name}\")\n\n            try:\n                before_data = query_datadog_timeseries(\n                    dd_api_key, dd_app_key, query_expression, before_start, before_end\n                )\n                after_data = query_datadog_timeseries(\n                    dd_api_key, dd_app_key, query_expression, after_start, after_end\n                )\n\n                before_series_values = extract_values_from_datadog_response(before_data)\n                after_series_values = extract_values_from_datadog_response(after_data)\n\n                eprint(f\"DEBUG: Before series: {list(before_series_values.keys())}\")\n                eprint(f\"DEBUG: After series: {list(after_series_values.keys())}\")\n\n                if not before_series_values or not after_series_values:\n                    eprint(f\"Skipping dashboard metric '{metric_name}': insufficient data\")\n                    continue\n\n                all_labels = sorted(set(before_series_values.keys()) | set(after_series_values.keys()))\n                eprint(f\"DEBUG: All labels to process: {all_labels}\")\n\n                for label in all_labels:\n                    before_values = before_series_values.get(label, [])\n                    after_values = after_series_values.get(label, [])\n\n                    eprint(f\"DEBUG: Processing label '{label}': before={len(before_values)} values, after={len(after_values)} values\")\n\n                    if not before_values or not after_values:\n                        eprint(f\"DEBUG: Skipping {label} - missing data\")\n                        continue\n\n                    before_p50 = calculate_percentile(before_values, 50)\n                    before_p99 = calculate_percentile(before_values, 99)\n                    after_p50 = calculate_percentile(after_values, 50)\n                    after_p99 = calculate_percentile(after_values, 99)\n\n                    metrics_analyzed += 1\n\n                    # Skip if baseline values are too small to calculate meaningful percentages\n                    if before_p50 \u003c 0.01 or before_p99 \u003c 0.01:\n                        eprint(f\"DEBUG: Skipping {label} - baseline values too small (P50: {before_p50:.4f}, P99: {before_p99:.4f})\")\n                        continue\n\n                    # Calculate percentage changes first\n                    p50_change_pct = ((after_p50 - before_p50) / before_p50 * 100)\n                    p99_change_pct = ((after_p99 - before_p99) / before_p99 * 100)\n\n                    # Check if BOTH P50 and P99 increased by at least 10%\n                    has_significant_increase = (p50_change_pct \u003e= 10.0 and p99_change_pct \u003e= 10.0)\n\n                    eprint(f\"DEBUG: {label} - P50: {before_p50:.2f} -\u003e {after_p50:.2f} ({p50_change_pct:+.1f}%), P99: {before_p99:.2f} -\u003e {after_p99:.2f} ({p99_change_pct:+.1f}%)\")\n                    eprint(f\"DEBUG: {label} - Significant increase (both \u003e= 10%): {has_significant_increase}\")\n\n                    if has_significant_increase:\n                        metrics_with_increases += 1\n\n                        p50_increase = after_p50 \u003e before_p50\n                        p99_increase = after_p99 \u003e before_p99\n\n                        result_metric_name = f\"{metric_name} ({label})\" if label != \"metric\" else metric_name\n\n                        result = {\n                            \"metric_name\": result_metric_name,\n                            \"base_metric_name\": metric_name,\n                            \"before_p50\": before_p50,\n                            \"before_p99\": before_p99,\n                            \"after_p50\": after_p50,\n                            \"after_p99\": after_p99,\n                            \"p50_change\": p50_change_pct,\n                            \"p99_change\": p99_change_pct,\n                            \"p50_increase\": p50_increase,\n                            \"p99_increase\": p99_increase,\n                            \"dashboard\": dashboard,\n                        }\n\n                        results_by_type[analysis_type].append(result)\n                        eprint(f\"  Found significant increase for {label} - P50: {before_p50:.2f} -\u003e {after_p50:.2f} ({p50_change_pct:+.1f}%), P99: {before_p99:.2f} -\u003e {after_p99:.2f} ({p99_change_pct:+.1f}%)\")\n\n                        if metric_name not in charts_generated:\n                            try:\n                                before_series_map = datadog_response_to_series(before_data)\n                                after_series_map = datadog_response_to_series(after_data)\n\n                                chart_buf = plot_continuous_with_release_boundary(\n                                    metric_name,\n                                    before_series_map,\n                                    after_series_map,\n                                    release_dt,\n                                    f\"{metric_name}\"\n                                )\n\n                                file_id = upload_image_to_slack_external(\n                                    chart_buf,\n                                    f\"{metric_name}_chart\",\n                                    title=f\"{metric_name} - Pre vs Post Release\",\n                                    channel_id=slack_channel_id,\n                                    slack_token=slack_bot_token\n                                )\n                                chart_file_data[metric_name] = file_id\n                                charts_generated.add(metric_name)\n                                eprint(f\"  Uploaded chart to Slack: {file_id}\")\n                            except Exception as chart_err:\n                                eprint(f\"  Warning: Failed to generate/upload chart for '{metric_name}': {chart_err}\")\n                    else:\n                        eprint(f\"  No significant increase detected for {label} (need both P50 and P99 \u003e= 10% increase)\")\n\n            except Exception as e:\n                eprint(f\"Error processing dashboard metric '{metric_name}': {e}\")\n                import traceback\n                eprint(traceback.format_exc())\n\n        blocks = generate_slack_blocks(\n            service_name, release_time, time_window_str, results_by_type, chart_file_data\n        )\n\n        summary_text = f\"Analyzed {metrics_analyzed} metrics for {service_name}. Found {metrics_with_increases} metrics with post-release increases.\"\n\n        return {\n            \"blocks\": blocks,\n            \"metrics_analyzed\": metrics_analyzed,\n            \"metrics_with_increases\": metrics_with_increases,\n            \"summary\": summary_text,\n            \"chart_file_data\": chart_file_data,\n        }\n\n    except Exception as e:\n        import traceback\n        eprint(traceback.format_exc())\n        return {\"success\": False, \"error\": str(e)}\n\n\nif __name__ == \"__main__\":\n    input_json = sys.stdin.read()\n    input_data = json.loads(input_json or \"{}\")\n    result = run(input_data)\n    print(json.dumps(result, indent=2))",
  "environmentVariables": [],
  "inputSchema": {
    "properties": {
      "metric_specs": {
        "description": "List of metrics to analyze, each with metric_name, query_expression, analysis_type, and optional tags",
        "type": "array"
      },
      "release_time": {
        "description": "ISO timestamp (RFC3339 format) marking the release time (e.g., '2024-01-15T10:30:00Z')",
        "type": "string"
      },
      "service_name": {
        "description": "Name of the service being analyzed",
        "type": "string"
      },
      "time_window": {
        "default": "1 hour",
        "description": "Time window for before/after comparison (e.g., '1 hour', '30 minutes')",
        "type": "string"
      }
    },
    "required": [
      "metric_specs",
      "service_name",
      "release_time"
    ],
    "type": "object"
  },
  "dependencies": {
    "enabled": true,
    "manager": "",
    "specText": "matplotlib\u003e=3.5.0\nnumpy\u003e=1.21.0\nrequests\u003e=2.28.0",
    "installPolicy": "",
    "installScope": "",
    "cacheEnabled": false,
    "lastInstall": {
      "status": "success",
      "installedAt": "2026-01-04T21:27:14Z",
      "cacheKey": "46dd0f0b8194d47dfefc74272b987232931d9c9d4405aa30ca9ed2a9ca71b316",
      "durationMs": 6599
    }
  },
  "execution": {
    "timeoutSeconds": 30,
    "workingDirectory": "workspace"
  },
  "createdAt": "2026-01-03T02:41:42Z",
  "updatedAt": "2026-01-16T01:17:21Z"
}