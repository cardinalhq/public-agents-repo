{
  "schemaVersion": 1,
  "name": "log-release-analysis-chart",
  "description": "",
  "language": "python",
  "code": "import sys\nimport json\nimport os\nimport re\nimport requests\nfrom datetime import datetime, timedelta, timezone\nimport matplotlib\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom io import BytesIO\n\n\nSTRICT_STDOUT_JSON_ONLY = True\n\n\ndef eprint(msg: str):\n    print(msg, file=sys.stderr)\n\n\ndef safe_filename(name: str, default: str = \"chart.png\") -\u003e str:\n    if not name:\n        return default\n    s = name.strip()\n    s = re.sub(r\"[\\/\\\\]+\", \"_\", s)\n    s = re.sub(r\"[^a-zA-Z0-9._()-]+\", \"_\", s)\n    s = s.strip(\"_\")\n    if not s:\n        return default\n    if not s.lower().endswith(\".png\"):\n        s += \".png\"\n    return s\n\n\ndef parse_iso8601(ts: str):\n    if not ts:\n        return None\n    ts = ts.replace(\"Z\", \"+00:00\")\n    dt = datetime.fromisoformat(ts)\n    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=timezone.utc)\n    return dt.astimezone(timezone.utc)\n\n\ndef safe_filename(name: str, default: str = \"chart.png\") -\u003e str:\n    if not name:\n        return default\n    s = name.strip()\n    s = re.sub(r\"[\\/\\\\]+\", \"_\", s)\n    s = re.sub(r\"[^a-zA-Z0-9._()-]+\", \"_\", s)\n    s = s.strip(\"_\")\n    if not s:\n        return default\n    if not s.lower().endswith(\".png\"):\n        s += \".png\"\n    return s\n\n\ndef upload_image_to_slack_external(image_buffer: BytesIO, filename: str, title: str = None, channel_id: str = None):\n    slack_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n    if not slack_token:\n        raise ValueError(\"Missing SLACK_BOT_TOKEN in payload or environment variables\")\n    \n    filename = safe_filename(filename)\n    image_buffer.seek(0)\n    img_bytes = image_buffer.read()\n    file_len = int(len(img_bytes))\n    \n    headers = {\"Authorization\": f\"Bearer {slack_token}\"}\n    \n    # Step 1: getUploadURLExternal (use data= form-encoded like your working example)\n    r1 = requests.post(\n        \"https://slack.com/api/files.getUploadURLExternal\",\n        headers=headers,\n        data={\"filename\": filename, \"length\": file_len},\n        timeout=30\n    )\n    j1 = r1.json()\n    if not j1.get(\"ok\"):\n        meta = j1.get(\"response_metadata\") or {}\n        msgs = meta.get(\"messages\") or []\n        if msgs:\n            eprint(f\"Slack getUploadURLExternal messages: {msgs}\")\n        eprint(f\"Slack getUploadURLExternal full response (first 1500): {json.dumps(j1, indent=2)[:1500]}\")\n        raise ValueError(f\"Slack getUploadURLExternal failed: {j1.get('error')}\")\n    \n    upload_url = j1[\"upload_url\"]\n    file_id = j1[\"file_id\"]\n    \n    # Step 2: upload to the returned URL\n    r2 = requests.post(\n        upload_url,\n        headers={\"Content-Type\": \"application/octet-stream\"},\n        data=img_bytes,\n        timeout=60\n    )\n    if r2.status_code \u003e= 300:\n        raise ValueError(f\"Upload to Slack external URL failed: {r2.status_code} {r2.text[:300]}\")\n    \n    # Step 3: completeUploadExternal (use data= and files as JSON string)\n    payload = {\"files\": json.dumps([{\"id\": file_id, \"title\": title or filename}])}\n    if channel_id:\n        payload[\"channel_id\"] = channel_id\n    \n    r3 = requests.post(\n        \"https://slack.com/api/files.completeUploadExternal\",\n        headers=headers,\n        data=payload,\n        timeout=30\n    )\n    j3 = r3.json()\n    if not j3.get(\"ok\"):\n        meta = j3.get(\"response_metadata\") or {}\n        msgs = meta.get(\"messages\") or []\n        if msgs:\n            eprint(f\"Slack completeUploadExternal messages: {msgs}\")\n        eprint(f\"Slack completeUploadExternal full response (first 1500): {json.dumps(j3, indent=2)[:1500]}\")\n        raise ValueError(f\"Slack completeUploadExternal failed: {j3.get('error')}\")\n    \n    # Return file_id for BlockKit slack_file reference\n    return file_id\n\n\ndef post_blocks_to_slack(blocks: list, channel_id: str):\n    \"\"\"\n    Posting requires BOTH token + channel_id.\n    \"\"\"\n    slack_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n    if not slack_token:\n        return False, \"Missing SLACK_BOT_TOKEN\"\n    if not channel_id:\n        return False, \"Posting disabled: Missing SLACK_CHANNEL_ID\"\n\n    try:\n        url = \"https://slack.com/api/chat.postMessage\"\n        headers = {\"Authorization\": f\"Bearer {slack_token}\", \"Content-Type\": \"application/json\"}\n        payload = {\"channel\": channel_id, \"blocks\": blocks}\n        eprint(f\"[slack] chat.postMessage channel={channel_id} blocks={len(blocks)}\")\n        resp = requests.post(url, headers=headers, json=payload, timeout=15)\n        resp.raise_for_status()\n        data = resp.json()\n        if data.get(\"ok\"):\n            return True, None\n        return False, data.get(\"error\", \"Unknown Slack error\")\n    except Exception as e:\n        return False, str(e)\n\n\n# ============================================================================\n# Local API helpers\n# ============================================================================\n\ndef normalize_logs_query(payload: dict) -\u003e dict:\n    p = dict(payload or {})\n    qt = (p.get(\"query_type\") or \"\").strip().lower()\n    p[\"query_type\"] = qt if qt in (\"events\", \"aggregate\") else \"aggregate\"\n    return p\n\n\ndef _extract_rows(payload):\n    if isinstance(payload, list):\n        return payload\n    if isinstance(payload, dict):\n        for k in (\"data\", \"result\", \"rows\", \"aggregates\", \"series\", \"points\"):\n            v = payload.get(k)\n            if isinstance(v, list):\n                return v\n        for k in (\"data\", \"result\"):\n            v = payload.get(k)\n            if isinstance(v, dict):\n                for kk in (\"rows\", \"aggregates\", \"series\"):\n                    vv = v.get(kk)\n                    if isinstance(vv, list):\n                        return vv\n    return []\n\n\ndef _parse_ts_any(ts_val):\n    if ts_val is None:\n        return None\n    if isinstance(ts_val, (int, float)):\n        tf = float(ts_val)\n        if tf \u003e 10_000_000_000:\n            tf /= 1000.0\n        return datetime.fromtimestamp(tf, tz=timezone.utc)\n    if isinstance(ts_val, str):\n        s = ts_val.strip()\n        if re.match(r\"^\\d+(\\.\\d+)?$\", s):\n            tf = float(s)\n            if tf \u003e 10_000_000_000:\n                tf /= 1000.0\n            return datetime.fromtimestamp(tf, tz=timezone.utc)\n        try:\n            dt = datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n            if dt.tzinfo is None:\n                dt = dt.replace(tzinfo=timezone.utc)\n            return dt.astimezone(timezone.utc)\n        except Exception:\n            return None\n    return None\n\n\ndef _pick_metric_value(metrics: dict):\n    if not isinstance(metrics, dict) or not metrics:\n        return None, None\n    for key in (\"count\", \"value\", \"sum\", \"avg\", \"p95\", \"p99\", \"max\", \"min\"):\n        v = metrics.get(key)\n        if isinstance(v, (int, float)):\n            return key, float(v)\n    for k, v in metrics.items():\n        if isinstance(v, (int, float)):\n            return str(k), float(v)\n    return None, None\n\n\ndef aggregates_to_points(rows):\n    pts = []\n    for r in rows or []:\n        if not isinstance(r, dict):\n            continue\n        group_by = r.get(\"groupBy\") or {}\n        metrics = r.get(\"metrics\") or {}\n\n        ts_raw = None\n        if isinstance(group_by, dict):\n            ts_raw = group_by.get(\"timestamp\") or group_by.get(\"time\") or group_by.get(\"ts\")\n\n        dt = _parse_ts_any(ts_raw)\n        if not dt:\n            continue\n\n        _, val = _pick_metric_value(metrics)\n        if val is None:\n            continue\n\n        pts.append({\"timestamp\": dt.timestamp(), \"value\": float(val)})\n\n    pts.sort(key=lambda x: x[\"timestamp\"])\n    dedup = {}\n    for p in pts:\n        dedup[p[\"timestamp\"]] = p[\"value\"]\n    return [{\"timestamp\": ts, \"value\": dedup[ts]} for ts in sorted(dedup.keys())]\n\n\ndef coerce_timeseries_points(payload_json):\n    if isinstance(payload_json, dict):\n        pts = payload_json.get(\"points\")\n        if isinstance(pts, list) and pts:\n            return pts\n    rows = _extract_rows(payload_json)\n    return aggregates_to_points(rows) if rows else []\n\n\ndef build_query_info(input_data: dict) -\u003e dict:\n    service_name = input_data[\"service_name\"]\n\n    pre_window = input_data.get(\"pre_window\") or {}\n    post_window = input_data.get(\"post_window\") or {}\n\n    start = input_data.get(\"query_start\") or pre_window.get(\"start\")\n    end = input_data.get(\"query_end\") or post_window.get(\"end\")\n    if not start or not end:\n        raise ValueError(\"Need pre_window.start and post_window.end (or query_start/query_end).\")\n\n    datasource_type = (input_data.get(\"datasource_type\") or \"datadog\").strip().lower()\n    step_seconds = int(input_data.get(\"step_seconds\") or 60)\n    query = input_data.get(\"query\") or f\"service:{service_name} Error\"\n\n    return {\n        \"datasource_type\": datasource_type,\n        \"start\": start,\n        \"end\": end,\n        \"step_seconds\": step_seconds,\n        \"query\": query,\n        \"query_type\": \"aggregate\",\n        \"group_by\": [\"level\"],\n    }\n\n\n# ============================================================================\n# Pattern extraction (supports your input shape)\n# ============================================================================\n\ndef extract_pattern_counts(window: dict) -\u003e dict:\n    \"\"\"\n    Supports:\n      - window.patterns: [{message,count}, ...]\n      - window.new_error_patterns: [{pattern,count}, ...]\n      - window.stopped_patterns: [{pattern,count}, ...]\n    \"\"\"\n    out = {}\n\n    # generic\n    if isinstance(window.get(\"patterns\"), list):\n        for p in window.get(\"patterns\") or []:\n            msg = p.get(\"message\") or p.get(\"pattern\")\n            cnt = p.get(\"count\")\n            if msg is None or cnt is None:\n                continue\n            out[str(msg)] = int(cnt)\n        return out\n\n    # your schema\n    for key in (\"new_error_patterns\", \"stopped_patterns\", \"all_patterns\", \"error_patterns\"):\n        if isinstance(window.get(key), list):\n            for p in window.get(key) or []:\n                msg = p.get(\"pattern\") or p.get(\"message\")\n                cnt = p.get(\"count\")\n                if msg is None or cnt is None:\n                    continue\n                out[str(msg)] = int(cnt)\n\n    return out\n\n\ndef compute_pattern_buckets(pre_window: dict, post_window: dict, min_pattern_count: int):\n    pre_patterns = extract_pattern_counts(pre_window)\n    post_patterns = extract_pattern_counts(post_window)\n\n    new_patterns = []\n    stopped_patterns = []\n    changed_patterns = []\n\n    # NEW: pre=0 -\u003e post\u003e=min\n    for msg, post_count in post_patterns.items():\n        pre_count = int(pre_patterns.get(msg, 0))\n        post_count = int(post_count)\n        if pre_count == 0 and post_count \u003e= min_pattern_count:\n            new_patterns.append({\"message\": msg, \"pre\": pre_count, \"post\": post_count})\n\n    # STOPPED: pre\u003e=min -\u003e post=0\n    for msg, pre_count in pre_patterns.items():\n        pre_count = int(pre_count)\n        post_count = int(post_patterns.get(msg, 0))\n        if post_count == 0 and pre_count \u003e= min_pattern_count:\n            stopped_patterns.append({\"message\": msg, \"pre\": pre_count, \"post\": post_count})\n\n    # CHANGED: both \u003e0, different, and big enough\n    for msg, post_count in post_patterns.items():\n        pre_count = int(pre_patterns.get(msg, 0))\n        post_count = int(post_count)\n        if pre_count \u003e 0 and post_count \u003e 0 and pre_count != post_count and max(pre_count, post_count) \u003e= min_pattern_count:\n            changed_patterns.append({\"message\": msg, \"pre\": pre_count, \"post\": post_count})\n\n    new_patterns.sort(key=lambda x: x[\"post\"], reverse=True)\n    stopped_patterns.sort(key=lambda x: x[\"pre\"], reverse=True)\n    changed_patterns.sort(key=lambda x: abs(x[\"post\"] - x[\"pre\"]), reverse=True)\n\n    return new_patterns, changed_patterns, stopped_patterns\n\n\n# ============================================================================\n# BlockKit pattern generation (replaces table images)\n# ============================================================================\n\ndef generate_pattern_blocks(section_title: str, rows: list) -\u003e list:\n    \"\"\"\n    Generate BlockKit blocks for patterns instead of table images.\n    \"\"\"\n    blocks = []\n\n    # Section header\n    blocks.append({\n        \"type\": \"section\",\n        \"text\": {\n            \"type\": \"mrkdwn\",\n            \"text\": f\"*{section_title}*\"\n        }\n    })\n\n    if not rows:\n        blocks.append({\n            \"type\": \"context\",\n            \"elements\": [\n                {\n                    \"type\": \"mrkdwn\",\n                    \"text\": \"_No patterns._\"\n                }\n            ]\n        })\n        return blocks\n\n    # Add each pattern as simple text\n    for r in rows:\n        msg = str(r.get(\"message\", \"\"))\n        if len(msg) \u003e 200:\n            msg = msg[:197] + \"…\"\n        \n        pre_c = int(r.get(\"pre\", 0))\n        post_c = int(r.get(\"post\", 0))\n        delta = post_c - pre_c\n        delta_str = f\"{delta:+d}\"\n\n        # Simple text format\n        text = f\"`{msg}`\\nPre: {pre_c} → Post: {post_c} (Δ {delta_str})\"\n\n        blocks.append({\n            \"type\": \"section\",\n            \"text\": {\n                \"type\": \"mrkdwn\",\n                \"text\": text\n            }\n        })\n\n    return blocks\n\n\n# ============================================================================\n# Image generators (timeseries only)\n# ============================================================================\n\ndef placeholder_image(title: str, message: str, width=12, height=3):\n    fig, ax = plt.subplots(figsize=(width, height))\n    ax.axis(\"off\")\n    ax.text(0.01, 0.70, title, fontsize=14, fontweight=\"bold\")\n    ax.text(0.01, 0.35, message, fontsize=11)\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", dpi=160, bbox_inches=\"tight\", facecolor=\"white\")\n    buf.seek(0)\n    plt.close(fig)\n    return buf\n\n\ndef render_timeseries_image(points: list, service_name: str, release_time: str, pre_window: dict, post_window: dict):\n    if not points:\n        return placeholder_image(\n            f\"Timeseries — {service_name}\",\n            \"No timeseries points returned from /api/v1/logs/query.\",\n            width=14,\n            height=3,\n        )\n\n    release_dt = parse_iso8601(release_time)\n    pre_start = parse_iso8601(pre_window[\"start\"])\n    pre_end = parse_iso8601(pre_window[\"end\"])\n    post_start = parse_iso8601(post_window[\"start\"])\n    post_end = parse_iso8601(post_window[\"end\"])\n\n    timestamps, values = [], []\n    for p in points:\n        ts, val = p.get(\"timestamp\"), p.get(\"value\")\n        if ts is None or val is None:\n            continue\n        tsf = float(ts)\n        if tsf \u003e 10_000_000_000:\n            tsf /= 1000.0\n        timestamps.append(datetime.fromtimestamp(tsf, tz=timezone.utc))\n        values.append(float(val))\n\n    if not timestamps or not values:\n        return placeholder_image(\n            f\"Timeseries — {service_name}\",\n            \"Points present but could not parse timestamp/value.\",\n            width=14,\n            height=3,\n        )\n\n    fig, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(timestamps, values, linewidth=2.2, linestyle=\"-\", alpha=0.95, label=\"Logs Count\")\n\n    ax.axvline(x=release_dt, color=\"red\", linestyle=\"--\", linewidth=2.6, label=\"Release\")\n    ax.axvspan(pre_start, pre_end, alpha=0.12, label=\"Pre-Release Window\")\n    ax.axvspan(post_start, post_end, alpha=0.12, label=\"Post-Release Window\")\n\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\", tz=timezone.utc))\n    plt.xticks(rotation=45)\n\n    ax.set_xlabel(\"Time (UTC)\", fontsize=12)\n    ax.set_ylabel(\"Log Count\", fontsize=12)\n    ax.set_title(f\"Log Timeseries — {service_name}\\nRelease: {release_time}\", fontsize=14, fontweight=\"bold\")\n    ax.legend(loc=\"upper right\")\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    buf = BytesIO()\n    plt.savefig(buf, format=\"png\", dpi=150, bbox_inches=\"tight\", facecolor=\"white\")\n    buf.seek(0)\n    plt.close(fig)\n    return buf\n\n\n# ============================================================================\n# Main run\n# ============================================================================\n\ndef run(input_data: dict):\n    try:\n        # required input\n        service_name = input_data[\"service_name\"]\n        release_time = input_data[\"release_time\"]\n        pre_window = input_data[\"pre_window\"]\n        post_window = input_data[\"post_window\"]\n        min_pattern_count = int(input_data.get(\"min_pattern_count\", 5))\n\n        # Slack: token REQUIRED, channel optional\n        slack_token = os.environ.get(\"SLACK_BOT_TOKEN\")\n        if not slack_token:\n            raise ValueError(\"Missing SLACK_BOT_TOKEN (required).\")\n\n        slack_channel = os.environ.get(\"SLACK_CHANNEL_ID\")  # optional\n        upload_enabled = True\n        post_enabled = bool(slack_channel)\n\n        if not slack_channel:\n            eprint(\"[slack] channel missing -\u003e uploads will be private; no chat.postMessage will be sent.\")\n\n        # local API env\n        local_api_base = (\n            input_data.get(\"local_api_base_url\")\n            or os.environ.get(\"LOCAL_API_BASE_URL\")\n            or \"http://localhost:20202\"\n        ).rstrip(\"/\")\n\n        local_api_token = os.environ.get(\"CARDINALHQ_API_KEY\")\n        dd_api_key = os.environ.get(\"DATADOG_API_KEY\")\n        dd_app_key = os.environ.get(\"DATADOG_APP_KEY\")\n        dd_site = os.environ.get(\"DATADOG_SITE\")\n\n        missing = [k for k, v in {\n            \"CARDINALHQ_API_KEY\": local_api_token,\n            \"DATADOG_API_KEY\": dd_api_key,\n            \"DATADOG_APP_KEY\": dd_app_key,\n        }.items() if not v]\n        if missing:\n            raise ValueError(f\"Missing required environment variables: {', '.join(missing)}\")\n\n        headers = {\n            \"Authorization\": f\"Bearer {local_api_token}\",\n            \"Content-Type\": \"application/json\",\n            \"DATADOG_API_KEY\": dd_api_key,\n            \"DATADOG_APP_KEY\": dd_app_key,\n        }\n        if dd_site:\n            headers[\"DATADOG_SITE\"] = dd_site\n\n        # logs query payload\n        query_info = input_data.get(\"warn_error_query_info\") or build_query_info(input_data)\n        payload = normalize_logs_query(query_info)\n\n        eprint(f\"[run] service={service_name} release={release_time} min_pattern_count={min_pattern_count}\")\n        eprint(f\"[run] local_api_base={local_api_base}\")\n        eprint(f\"[run] logs payload: {json.dumps(payload, indent=2)[:1800]}\")\n\n        # 1) PATTERNS\n        new_patterns, changed_patterns, stopped_patterns = compute_pattern_buckets(pre_window, post_window, min_pattern_count)\n        eprint(f\"[patterns] new={len(new_patterns)} changed={len(changed_patterns)} stopped={len(stopped_patterns)}\")\n\n        # 2) TIMESERIES\n        points = []\n        timeseries_error = None\n        try:\n            timeseries_url = f\"{local_api_base}/api/v1/logs/query\"\n            eprint(f\"[timeseries] POST {timeseries_url}\")\n            resp = requests.post(timeseries_url, headers=headers, json=payload, timeout=30)\n            eprint(f\"[timeseries] status={resp.status_code}\")\n            if resp.status_code != 200:\n                eprint(f\"[timeseries] response (first 1200): {resp.text[:1200]}\")\n            resp.raise_for_status()\n            raw = resp.json()\n            eprint(f\"[timeseries] response sample (first 900): {json.dumps(raw)[:900]}\")\n            points = coerce_timeseries_points(raw)\n            eprint(f\"[timeseries] points={len(points)}\")\n        except Exception as e:\n            timeseries_error = str(e)\n            eprint(f\"[timeseries] ERROR: {timeseries_error}\")\n\n        # 3) Generate timeseries image (only image)\n        img_timeseries = render_timeseries_image(points, service_name, release_time, pre_window, post_window)\n\n        # 4) Upload timeseries image\n        file_ids = {\"timeseries\": None}\n        upload_errors = {}\n\n        try:\n            file_ids[\"timeseries\"] = upload_image_to_slack_external(\n                img_timeseries,\n                filename=safe_filename(f\"logs_timeseries_{service_name}\", default=\"logs_timeseries.png\"),\n                title=f\"Timeseries — {service_name}\",\n                channel_id=slack_channel,\n            )\n        except Exception as e:\n            upload_errors[\"timeseries\"] = str(e)\n\n        # 5) Build blocks (timeseries image + BlockKit patterns)\n        used_query = payload.get(\"query\", \"N/A\")\n\n        blocks = [\n            {\"type\": \"header\", \"text\": {\"type\": \"plain_text\", \"text\": f\"Logs Release Analysis — {service_name}\"}},\n            {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": f\"*Query:* `{used_query}`\"}},\n            {\"type\": \"context\", \"elements\": [{\"type\": \"mrkdwn\", \"text\": f\"*Release:* `{release_time}`  |  *Min pattern count:* `{min_pattern_count}`\"}]},\n            {\"type\": \"divider\"},\n        ]\n\n        # Add timeseries image\n        if file_ids[\"timeseries\"]:\n            blocks.append({\"type\": \"image\", \"slack_file\": {\"id\": file_ids[\"timeseries\"]}, \"alt_text\": f\"Timeseries — {service_name}\"})\n            blocks.append({\"type\": \"divider\"})\n\n        # Add pattern blocks (BlockKit text, not images)\n        if new_patterns:\n            blocks.extend(generate_pattern_blocks(\"NEW ERROR PATTERNS\", new_patterns))\n            blocks.append({\"type\": \"divider\"})\n\n        if changed_patterns:\n            blocks.extend(generate_pattern_blocks(\"CHANGED PATTERNS\", changed_patterns))\n            blocks.append({\"type\": \"divider\"})\n\n        if stopped_patterns:\n            blocks.extend(generate_pattern_blocks(\"STOPPED PATTERNS\", stopped_patterns))\n            blocks.append({\"type\": \"divider\"})\n\n        # 6) Post message only if channel provided\n        ok, err = (False, \"Posting disabled (missing SLACK_CHANNEL_ID)\")\n        if post_enabled:\n            ok, err = post_blocks_to_slack(blocks, slack_channel)\n            if not ok:\n                eprint(f\"[slack] post_blocks ERROR: {err}\")\n\n        summary = {\n            \"service_name\": service_name,\n            \"release_time\": release_time,\n            \"min_pattern_count\": min_pattern_count,\n            \"patterns\": {\n                \"new_count\": len(new_patterns),\n                \"changed_count\": len(changed_patterns),\n                \"stopped_count\": len(stopped_patterns),\n                \"new_top_3\": new_patterns[:3],\n                \"changed_top_3\": changed_patterns[:3],\n                \"stopped_top_3\": stopped_patterns[:3],\n            },\n            \"timeseries\": {\"points\": len(points), \"error\": timeseries_error},\n            \"slack\": {\n                \"token_present\": True,\n                \"channel_id\": slack_channel,\n                \"upload_enabled\": upload_enabled,\n                \"post_enabled\": post_enabled,\n                \"post_ok\": ok,\n                \"post_error\": err,\n            },\n            \"uploads\": {\"file_ids\": file_ids, \"errors\": upload_errors},\n            \"query_info_used\": payload,\n        }\n\n        print(json.dumps({\"blocks\": blocks, \"summary\": summary}, indent=2))\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc(file=sys.stderr)\n        print(json.dumps({\"success\": False, \"error\": str(e)}))\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    input_json = sys.stdin.read()\n    input_data = json.loads(input_json or \"{}\")\n    run(input_data)",
  "environmentVariables": [],
  "inputSchema": {
    "properties": {
      "local_api_base_url": {
        "description": "Optional override for local API base URL (default: http://localhost:20202)",
        "type": "string"
      },
      "min_pattern_count": {
        "description": "Minimum count threshold for patterns to be considered (default: 5)",
        "type": "integer"
      },
      "post_window": {
        "description": "Post-release time window with log patterns",
        "type": "object"
      },
      "pre_window": {
        "description": "Pre-release time window with log patterns",
        "type": "object"
      },
      "release_time": {
        "description": "ISO 8601 timestamp of the release (e.g., 2025-12-31T19:00:00Z)",
        "type": "string"
      },
      "service_name": {
        "description": "Name of the service being analyzed",
        "type": "string"
      }
    },
    "required": [
      "service_name",
      "release_time",
      "pre_window",
      "post_window"
    ],
    "type": "object"
  },
  "dependencies": {
    "enabled": true,
    "manager": "pip",
    "specText": "requests\u003e=2.28.0\nmatplotlib\u003e=3.5.0\nnumpy\u003e=1.21.0",
    "installPolicy": "on_save",
    "installScope": "per_script",
    "cacheEnabled": false,
    "lastInstall": {
      "status": "success",
      "installedAt": "2026-01-04T22:19:55Z",
      "cacheKey": "46dd0f0b8194d47dfefc74272b987232931d9c9d4405aa30ca9ed2a9ca71b316",
      "durationMs": 6985
    }
  },
  "execution": {
    "timeoutSeconds": 30,
    "workingDirectory": "workspace"
  },
  "createdAt": "2025-12-31T19:56:12Z",
  "updatedAt": "2026-01-14T22:50:54Z"
}